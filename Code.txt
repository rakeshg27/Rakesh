
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

spark = SparkSession.builder \
    .appName("Spark-program") \
    .master("local[*]") \
    .getOrCreate()

orderData = [
    ("Order1", "John", 100),
    ("Order2", "Alice", 200),
    ("Order3", "Bob", 150),
    ("Order4", "Alice", 300),
    ("Order5", "Bob", 250),
    ("Order6", "John", 400)
]


schema = StructType([
    StructField("OrderID", StringType(), True),
    StructField("Customer", StringType(), True),
    StructField("Amount", IntegerType(), True),  # Added True for nullability
])

df = spark.createDataFrame(orderData, schema)

df.show()




[16/06, 7:04 pm] +91 99894 54737: val df = List(("2023-10-07", "2023-10-10")).toDF("date1", "date2")
[16/06, 7:05 pm] +91 99894 54737: : Calculate the number of days between two dates
[16/06, 7:05 pm] +91 99894 54737: Given a DataFrame with date1 and date2 columns, handle missing date values
by filling them with default dates.
[16/06, 7:05 pm] +91 99894 54737: val df = List(("2023-10-07", null), (null, "2023-10-08")).toDF("date1", "date2")





mylist=[(
 ("Alice", "Math", 80),
 ("Bob", "Math", 90),
 ("Alice", "Science", 70),
 ("Bob", "Science", 85),
 ("Alice", "English", 75),
 ("Bob", "English", 95)
)]



