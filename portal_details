Seekho big data
JUW25Z8VVr2tgn

https://deloitte.zoom.us/j/97022029058?pwd=amdQenBzZW1LMkR4RDgwc0RHSW9lUT09

https://meet.google.com/ipg-zwzv-ygt

https://worker-sparkling-meadow-2f92.yotebo8338.workers.dev/893de0167c0e457a268bd581ecbf4d52bc8de49359e45631161157fe8ebf95bcc7d6eaec87b4f2dce0555507c0eaba2409d0dc41310a82fa76c51a0c4fc8eaa17be5fe6b25ed1d291c57da339f07c8d491f93caa85d77e3d5c4a31bcc9f9d1d5be7897e727a2f16c201f625ec8b64d737d2b2e8e3e18f379d3da1b85711b426ce5172d6f2e44b81c93240024aea1848b04c64b5a2484e565ed91a02ebdf53bee293af56eda44e62fe633fb8fbabe47cf::4e54b109810c3c66079875687af8ddc7/Mirzapur.S03.1080p.AMZN.WEBDL.DDP5.1.H.264TopMovies.tel.zip

Delloit Interview:
https://medium.com/@ronitmalhotraofficial/deloitte-pyspark-interview-questions-for-data-engineer-2024-9bad784e0a92


UAN: 101695361595
Azure CG portal:
https://portal.azure.com/?quickstart=true#home
https://meet.google.com/btu-nxkz-rrf?authuser=0

AZURE 
excel file for test:
https://docs.google.com/spreadsheets/d/1JMcznNMC0tCc-OaniraZ0GYGP7alg828j5aK-pgTRRM/edit?usp=sharing

-- Main entry point of Databricks learning journey 
https://medium.com/the-data-therapy/how-i-scored-95-on-the-databricks-data-engineer-associate-certification-a-comprehensive-guide-c4ea47485a05
https://ad.doubleclick.net/ddm/clk/585382003;393693070;j;dc_transparent=1;?https://jobs.citi.com/job/-/-/287/66539626224?utm_term=393693070&ss=paid&utm_campaign=apac_experienced&utm_medium=job_posting&source=linkedinJB&utm_source=linkedin.com&utm_content=social_media

https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZTZlYzA5ZjctOTZmNi00OTRhLTkyZDEtNDllYzMxMmMyYzE3%40thread.v2/0?context=%7b%22Tid%22%3a%22404b1967-6507-45ab-8a6d-7374a3f478be%22%2c%22Oid%22%3a%222564f182-8f2a-4fd1-8252-d6026fedb3c5%22%7d

This is to store my personal information:
refferal -- fractal om.rakesh27@gmail.com Kishan@123
https://fractal.wd1.myworkdayjobs.com/Careers/job/Bengaluru/Azure-Data-Engineer_SR-18098?shared_id=YzM1ODRlOWMtZDIwYi00OWU5LTk0YzMtNGZkZWNiOGVjNjVk
https://www.hackerearth.com/azure-data-engineer-test-maha
https://www.youtube.com/@cleverstudies/playlists --Data Engineer 
https://www.youtube.com/playlist?list=PLCLE6UVwCOi1r-LnSDyM-efVK9NpWQMEt  --pyspark by Naresh
https://www.youtube.com/watch?v=_wJ_8E6VumU&list=PLz-qytj7eIWXTqncmCCSOw-GcBu2c4K0j&ab_channel=DataScienceBasics  --Databrickss in 30 Days
https://www.youtube.com/@rajasdataengineering7585/playlists  -- Raja Data Engineering [ databricks pyspark mostly coding explain]
https://www.machinelearningplus.com/pyspark/pyspark-exercises-101-pyspark-exercises-for-data-analysis/   --pyspark practice 

https://acnrecruitment.accenture.com/interview/#/home/eyJsdCI6ICJUa246OTdhZTBiMzYtZjMyMC00ZWUzLWFkNTktMGIzYTBhMTdkNjEyIn0=

https://acnrecruitment.accenture.com/interview/#/home/eyJsdCI6ICJUa246OTdhZTBiMzYtZjMyMC00ZWUzLWFkNTktMGIzYTBhMTdkNjEyIn0=
https://accenture.wd3.myworkdayjobs.com/AccentureCareers/login?Job_Application_ID=8d50a394d8cd900b01e0e3000ab00000
https://accenture.wd3.myworkdayjobs.com/AccentureCareers/userHome/
https://doccollectionpartner4001.accenture.com/

https://meet.google.com/pta-miho-gpf
https://app.codesignal.com/public-test/2TzHgr3k5ZxjorQfy/4t9uuth3N73sCS


1)Successfully utilized Apache Spark for batch data processing, reducing processing time by 30% and improving data accuracy for critical business insights. 2)Leveraged Hive to manage and optimize structured data, resulting in a 25% improvement in query performance and enhanced reporting capabilities. 3Implemented data transfer workflows using Sqoop, ensuring seamless and reliable ETL processes between Hadoop and relational databases. 4)Developed Scala applications for data processing, contributing to the creation of efficient and scalable solutions within the Big Data landscape. 5)Proficiently crafted SQL queries to analyze and transform data, enabling data-driven decision-making and supporting strategic business initiatives. 6)Orchestrated CI/CD pipelines with Jenkins, automating software delivery and deployment processes, leading to faster release cycles and reduced errors. 7)Utilized Docker for containerization, deployment and ensuring consistent behavior across dev.



from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
from pyspark.sql.functions import col, when, count, initcap

spark = SparkSession.builder \
    .appName("Spark-program") \
    .master("local[*]") \
    .getOrCreate()

# schema = "id Int,Name String,Age Int,Salary Int, city String"

schema = StructType([
    StructField("id", IntegerType()),
    StructField("Name", StringType()),
    StructField("Age", IntegerType()),
    StructField("Salary", IntegerType()),
    StructField("city", StringType()),

])

df = spark.read.csv(<>)
df = spark.read \
    .format("csv") \
    .option("header", True) \
    .schema(schema) \
    .option("mode", "FAILFAST") \
    .option("path", "C:/Users/Karthik Kondpak/Documents/info.csv") \
    .load()

# orderData = spark.createDataFrame([
#     ("Order1", "John", 100),
#     ("Order2", "Alice", 200),
#     ("Order3", "Bob", 150),
#     ("Order4", "Alice", 300),
#     ("Order5", "Bob", 250),
#     ("Order6", "John", 400)
# ], ["OrderID", "Customer", "Amount"])

# df.select(col("id"), col("Name"), when((col("Age") > 30) and col("Name").startswith("A"), "JUNIORS").otherwise("Seniors").alias("details")).show()

df.select(
    col("id"),
    initcap(col("Name")),
    col("Age"),

).show()

# &---bitwise -1010010
# and--logical


df.groupby("city").agg(count(col("Name"))).show()

# df.show()

df.printSchema()



val mylist=List(("ProductA","CategoryX","2023-09-01",100.0),
("ProductA","CategoryX","2023-09-02",120.0)
("ProductA","CategoryX","2023-09-03",130.0)
("ProductB","CategoryY","2023-09-01",200.0)
("ProductB","CategoryY","2023-09-02",210.0))


WITH battle_results AS (
    SELECT 
        b.battle_number,
        b.region,
        CASE 
            WHEN b.attacker_outcome = 'win' THEN k_attacker.house
            ELSE k_defender.house
        END AS winning_house
    FROM 
        battle b
    JOIN 
        king k_attacker ON b.attacker_king = k_attacker.kno
    JOIN 
        king k_defender ON b.defender_king = k_defender.kno
)
SELECT 
    region,
    winning_house AS house,
    COUNT(*) AS wins
FROM 
    battle_results
GROUP BY 
    region, winning_house
ORDER BY 
    region, house;










https://assess.wecreateproblems.com/tests/a4e019ed-e661-4cca-991e-bd12e6b7aea1/instructions

refferal -- EY om.rakesh27@gmail.com Kishan@123
https://career5.successfactors.eu/career?company=EYHRISPRD1&login%5fns=pwd%5fset&account=V4%2d0%2da1%2d1RUuvvgMeemvR%2dPFIHudmfGQ4Gw4zzyIuVy%2dUzTV%2dFxbndIKdwu7MxZsrr3Yue3%2doUKmoUDjzFVH2I%5ftBjA3LrbJH2Zf6dPrpLP6AcH%5f0MQICco&verify=mMbEsVNDw27I6BEGQSOlOfN%2bL8etxTp8WtshtOkoReo%3d&loginFlowRequired=true&rcm%5fsite%5flocale=en%5fUS&career%5femail%5furl=true&jobReqId=1460240&

refferal Note:
Strongly recommend Rakesh Gupta for the Big Data Engineer role.he is Big Data Engineer with 3 years of experience in designing, developing, and maintaining scalable data solutions
using Hadoop, Hive-SQL, Data Warehouse, PySpark, Azure Databricks, Teradata, Snowflake, Python and excel at data analysis, problem-solving, and communication. Possesses a strong understanding of
data structures, SQL concepts, and complex SQL scripting with expertise in building and optimizing data pipelines for high-volume data processing.
Proven ability to collaborate effectively with cross-functional teams to deliver innovative solutions and ensure high customer satisfaction.
He is a great fit for our team!

Databricks link:
https://community.cloud.databricks.com/?o=4045616047254302#notebook/2399337524275070/command/1073200127988663

https://meet.google.com/qnf-rkna-jtz

https://intapidm.infosysapps.com/auth/realms/careersite/protocol/openid-connect/auth?client_id=careersite&redirect_uri=https%3A%2F%2Fcareer.infosys.com%2Fjobs%2FjobsStatus%3Fcompanyhiringtype%3DIL%26countrycode%3DIN%26searchJob%3Dundefined&state=7807201c-e5dd-4a1a-a571-c423bac5aee5&response_mode=fragment&response_type=code&scope=openid&nonce=f851c158-798d-4d6f-8cc3-faa110b6a71b

https://github.com/sdmg15/Best-websites-a-programmer-should-visit

https://novoresume.com/t/s/d02ux610qvifge8i2uok8mz98x6nn30ju5ddidpww7bnoh3gju7rh9h3ampz7bhkxnyydj1xw4g6vl3mjlvk672vsi2nsAt8d7m656du871n345g6bz3ogrte234oespiAge3sen6iqkhh5hts79revn867rb17wxc7etxziisi58kqg5nzdegfc9jjvnr5l3wj5391ff1c80105311efA842b91fe6c02bf0

JOB NO. ATCI-4314415-S1672893 

interview questions 

Big Data Engineer frequently ask interview questions:

1. Kindly explain your project architecture?

2. Day to Day activity in office

3. What is the size of data you deal with on daily basis?

4. What is Repartition and Coalesce?

5. What optimisation techniques have you used in your project?

6. What is your role in your project?

7. What is the most challenging problem you have solved in your big data project?

8. Can you explain what happens internally when we submit a Spark job using

Spark-Submit?

9. What is a catalyst optimiser?

10. What is the size of your Spark cluster and the configuration of each node?

11. How to tune a spark job? Please explain the techniques we can try.

12. What is Repartition and Coalesce?

13. What is Spark Context vs Spark Session?

14. Role in your current project

15. Difference between dataset & dataframe

16. Difference between broadcast variable & accumulator

17. What is broadcast Join?

18. Types of transformation and difference

19. What are operations of data frames

20. Explain Spark on Yarn Architecture

21. Why reduce is action & reduceByKey transformation

22. What is cache & persist in Spark

23. Difference between RDD & Dataframes

24. What are the challenges you face in spark?

25. How spark is better than Hive?

26. How to enforce schema on a data frame?

27. What is difference between reduceByKey & groupByKey?

28. How do we subrnit jar files in Spark?


1. Why Spark is Faster Than Hadoop? Hadoop Vs spark
2. Which language to choose and Why? Scala vs Python
3. Explain about the Apache Spark Architecture
4. What do you understand by Spark Execution Model
5. Brief about spark internals, Spark Session vs Spark Context
6. Spark Driver vs Spark Executor
7. Executor vs Executor core
8. Yarn client mode vs cluster mode
9. What is RDD and what do you understand by partitions?
10. What do you understand by Fault tolerance in Spark?
11. Spark vs Yarn Fault tolerance
12. Why Lazy evaluation is important in Spark?
13. Transformations vs actions
14. Map vs FlatMap
15. Spark Map vs Map Partition
16. Wide vs Narrow transformations
17. Reduce by key vs Group by key
18. What do you understand by Spark Lineage
19. Spark Lineage vs Spark DAG
20. Spark cache vs Spark persist
21. What do you understand by AggregateByKey and CombineByKey?
22. Briefly explain about Spark Accumulator
23. What do you mean by Broadcast variables?
24. Spark UDF functions, Why one should avoid UDF?
25. Why one should avoid RDDs, what is the alternative?
26. What are the benefits of a data frame?
27. What do you understand by Vectorized UDF?
28. Which one is better and when you should use, RDDs, Dataframe and Datasets?
29. Why Spark Dataset is typesafe?
30. Explain about Repartition and Coalesce.
31. How to read JSON from Spark?
32. Explain about Spark WIndow functions and it’s usage.
33. Spark Rank vs Dense Rank
34. Partitions vs Bucketing
35. Explain about catalyst optimizer
36. Stateless vs Stateful transformations
37. StructType and StructField
38. Explain about Apache parquet
39. What do you understand by CBO, Spark Cost Based Optimizer?
40. Explain Broadcast variable and shared variable with examples
41. Have you ever worked on Spark performance tuning and executor tuning
42. Explain Spark Join without shuffle
43. Explain about Paired RDD
44. Cache vs Persist in Spark UI
45. Why one should avoid groupBy?
46. How to decide the number of partitions in a data frame?
47. What is DAG? Explain in details.
48. Persistence vs Broadcast in Spark
49. Partition pruning and predicate pushdown
50. Fold vs reduce in Spark
51. Explain the interlinking of Pyspark and Apache Arrow
52. Explain about bucketing in Spark SQL
53. Explain dynamic resource allocation in Spark
54. Why fold-left and fold-right are not supported in Spark?
55. How to decide the number of executors and memory for any spark job?
56. Different types of cluster managers in spark
57. Can you explain how to minimize data transfers while working with Spark?
58. What are the different levels of persistence in Spark?
59. What is the function of filer()?
60. Define Partitions in Apache Spark?
61. What is the difference between reducing () and take() function?
62. Define YARN in Spark?
63. Can we trigger automated clean-ups in Spark?
64. What is another method than “Spark.cleaner.ttl” to trigger automated clean-ups in Spark?
65. What is the role of Akka in Spark?
66. Define SchemaRDD in Apache Spark RDD
67. What is a Spark Driver?


https://wise-live.zoom.us/j/92450622433?pwd=MlJLL256ZUpTWjhaeDFObHJXMWF5Zz09

https://wise-live.zoom.us/j/92076541694?pwd=a29WYmlsdGlBZVVBZ2F5SVBKbHg0Zz09

Click https://wise-live.zoom.us/j/92076541694?pwd=a29WYmlsdGlBZVVBZ2F5SVBKbHg0Zz09 to join a Zoom meeting.



